{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an expanded pytest test suite that demonstrates both unit tests (mocked) and a real integration test against an actual S3 location. It also provides examples of negative tests to handle edge cases, like a non-existent bucket, prefix, or missing file.\n",
    "\n",
    "Important:\n",
    "\n",
    "Integration Tests require valid AWS/S3 credentials and network access to an S3 endpoint.\n",
    "For local testing with MinIO or a custom S3 endpoint, you must configure your environment variables accordingly (e.g., AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_DEFAULT_REGION, S3_ENDPOINT_URL, etc.).\n",
    "Make sure PySpark is configured to access your S3 endpoint (via spark._jsc.hadoopConfiguration().set(...) or the spark-submit CLI configs).\n",
    "Sample test_s3_read.py with Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "from unittest.mock import MagicMock\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from s3 import (\n",
    "    read_from_s3_spark,\n",
    "    _create_df_daterange,\n",
    "    _parse_raw_json,\n",
    "    _apply_filters,\n",
    "    init_s3_spark\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                               Pytest Fixtures\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    \"\"\"\n",
    "    A pytest fixture that provides a SparkSession for tests.\n",
    "    Runs at 'session' scope so it’s created once per test run.\n",
    "    \"\"\"\n",
    "    # Example local Spark, for demonstration:\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"PytestSpark\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Optionally configure S3 credentials in Spark here if needed:\n",
    "    # spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", os.environ.get(\"AWS_ACCESS_KEY_ID\"))\n",
    "    # spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", os.environ.get(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "    # spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", os.environ.get(\"S3_ENDPOINT_URL\", \"s3.amazonaws.com\"))\n",
    "    # spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_s3_spark(spark_session):\n",
    "    \"\"\"\n",
    "    A fixture that returns a mock or a real SparkSession configured for S3 usage.\n",
    "    If you want to test local mocking, you can replace this with a MagicMock.\n",
    "    Or if you want to test real S3 connectivity, use init_s3_spark with real creds.\n",
    "    \"\"\"\n",
    "    # Example: If you want a real Spark with init_s3_spark:\n",
    "    #\n",
    "    # s3_spark = init_s3_spark(\n",
    "    #     endpoint=os.environ[\"S3_ENDPOINT_URL\"],\n",
    "    #     s3_access_key=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    #     s3_secret_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "    #     spark_home=\"/path/to/spark/home\",\n",
    "    #     spark_remote=None,   # or \"spark://...\" depending on your setup\n",
    "    #     ca_certs=None,\n",
    "    #     session_name=\"PytestSession\"\n",
    "    # )\n",
    "    #\n",
    "    # return s3_spark\n",
    "    #\n",
    "    # For now, return the existing local session:\n",
    "    return spark_session\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                           Unit Tests (Mocked)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def test_read_from_s3_spark_no_select_no_filters_no_parse(spark_session, mocker):\n",
    "    \"\"\"\n",
    "    Test read_from_s3_spark with minimal arguments: \n",
    "    no 'select', no 'filters', and no 'parse_column'.\n",
    "    \"\"\"\n",
    "    # Create a stub DataFrame\n",
    "    schema = StructType([StructField(\"dummy_col\", StringType(), True)])\n",
    "    df_stub = spark_session.createDataFrame([(\"test_val\",)], schema=schema)\n",
    "\n",
    "    # Mock _create_df_daterange to return our df_stub\n",
    "    mock_create_df = mocker.patch(\"s3._create_df_daterange\", return_value=df_stub)\n",
    "\n",
    "    # Call the function under test\n",
    "    result_df = read_from_s3_spark(\n",
    "        s3_spark=spark_session,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\",\n",
    "        start_date=datetime(2021, 1, 1),\n",
    "        end_date=datetime(2021, 1, 2),\n",
    "        select=None,\n",
    "        filters=None,\n",
    "        parse_column=None\n",
    "    )\n",
    "\n",
    "    # Assertions\n",
    "    assert mock_create_df.called, \"Expected _create_df_daterange to be called.\"\n",
    "    assert result_df.schema == df_stub.schema, \"Result schema should match the stub schema.\"\n",
    "    assert result_df.count() == 1, \"Result dataframe should have one row.\"\n",
    "    row = result_df.collect()[0]\n",
    "    assert row['dummy_col'] == \"test_val\", \"Row value should match the stubbed value.\"\n",
    "\n",
    "def test_read_from_s3_spark_with_select(spark_session, mocker):\n",
    "    \"\"\"\n",
    "    Test read_from_s3_spark with a 'select' list of columns.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"dummy_col\", StringType(), True),\n",
    "        StructField(\"another_col\", StringType(), True)\n",
    "    ])\n",
    "    df_stub = spark_session.createDataFrame(\n",
    "        [(\"test_val\", \"another_val\")],\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    mock_create_df = mocker.patch(\"s3._create_df_daterange\", return_value=df_stub)\n",
    "\n",
    "    # We want only 'dummy_col'\n",
    "    select_cols = [\"dummy_col\"]\n",
    "    result_df = read_from_s3_spark(\n",
    "        s3_spark=spark_session,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\",\n",
    "        start_date=datetime(2021, 1, 1),\n",
    "        end_date=datetime(2021, 1, 2),\n",
    "        select=select_cols,\n",
    "        filters=None,\n",
    "        parse_column=None\n",
    "    )\n",
    "\n",
    "    # Verify\n",
    "    assert mock_create_df.called\n",
    "    assert len(result_df.columns) == 1, \"Result should have exactly 1 column from 'select'.\"\n",
    "    assert \"dummy_col\" in result_df.columns, \"Result must include 'dummy_col'.\"\n",
    "    assert \"another_col\" not in result_df.columns, \"'another_col' should be excluded by the select.\"\n",
    "\n",
    "def test_read_from_s3_spark_with_parse_column(spark_session, mocker):\n",
    "    \"\"\"\n",
    "    Test read_from_s3_spark when parse_column is provided.\n",
    "    Ensures _parse_raw_json is called.\n",
    "    \"\"\"\n",
    "    df_stub_initial = spark_session.createDataFrame(\n",
    "        [(\"raw_json_data\",)], \n",
    "        schema=StructType([StructField(\"raw_json\", StringType(), True)])\n",
    "    )\n",
    "    df_stub_parsed = spark_session.createDataFrame(\n",
    "        [(\"extracted_val\",)], \n",
    "        schema=StructType([StructField(\"parsed_col\", StringType(), True)])\n",
    "    )\n",
    "\n",
    "    mock_create_df = mocker.patch(\"s3._create_df_daterange\", return_value=df_stub_initial)\n",
    "    mock_parse_json = mocker.patch(\"s3._parse_raw_json\", return_value=df_stub_parsed)\n",
    "\n",
    "    result_df = read_from_s3_spark(\n",
    "        s3_spark=spark_session,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\",\n",
    "        start_date=datetime(2021, 1, 1),\n",
    "        end_date=datetime(2021, 1, 2),\n",
    "        select=[\"parsed_col\"],\n",
    "        filters={},\n",
    "        parse_column=\"raw_json\"\n",
    "    )\n",
    "\n",
    "    # Check if _parse_raw_json was called with the right args\n",
    "    mock_parse_json.assert_called_once()\n",
    "    called_args, _ = mock_parse_json.call_args\n",
    "    # called_args[0] = df, called_args[1] = select (list), called_args[2] = filters, called_args[3] = column_to_parse\n",
    "    assert called_args[0].schema == df_stub_initial.schema, \\\n",
    "        \"Should pass the original DF to _parse_raw_json.\"\n",
    "\n",
    "    # The final DataFrame should come from the parsed stub\n",
    "    assert result_df.schema == df_stub_parsed.schema\n",
    "    assert result_df.collect()[0]['parsed_col'] == \"extracted_val\"\n",
    "\n",
    "def test_read_from_s3_spark_with_filters(spark_session, mocker):\n",
    "    \"\"\"\n",
    "    Test read_from_s3_spark with filters. Ensures _apply_filters is used.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"dummy_col\", StringType(), True),\n",
    "        StructField(\"filter_col\", StringType(), True)\n",
    "    ])\n",
    "    df_stub = spark_session.createDataFrame(\n",
    "        [\n",
    "            (\"keep_row\", \"match\"),\n",
    "            (\"discard_row\", \"no_match\")\n",
    "        ],\n",
    "        schema=schema\n",
    "    )\n",
    "    mock_create_df = mocker.patch(\"s3._create_df_daterange\", return_value=df_stub)\n",
    "\n",
    "    # We'll filter where filter_col == 'match'\n",
    "    filters = {\"filter_col\": \"match\"}\n",
    "    result_df = read_from_s3_spark(\n",
    "        s3_spark=spark_session,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\",\n",
    "        start_date=datetime(2021, 1, 1),\n",
    "        end_date=datetime(2021, 1, 2),\n",
    "        select=None,\n",
    "        filters=filters,\n",
    "        parse_column=None\n",
    "    )\n",
    "\n",
    "    assert mock_create_df.called\n",
    "    # Should only keep the row matching filter_col == \"match\"\n",
    "    assert result_df.count() == 1, \"Only one row should remain after filter.\"\n",
    "    row = result_df.collect()[0]\n",
    "    assert row[\"dummy_col\"] == \"keep_row\", \"The row with 'discard_row' should be filtered out.\"\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                Direct Tests for Helper Methods (Optional)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def test__create_df_daterange_single_date(spark_session, mocker):\n",
    "    from s3 import _create_df_daterange\n",
    "    mocker.patch(\"s3.get_text_before_date\", return_value=\"date_label=\")\n",
    "\n",
    "    mock_s3_spark = MagicMock()\n",
    "    df_stub = spark_session.createDataFrame([(\"test\",)], [\"col1\"])\n",
    "    mock_s3_spark.read.parquet.return_value = df_stub\n",
    "\n",
    "    single_date = [datetime(2021, 1, 1)]\n",
    "    result_df = _create_df_daterange(\n",
    "        s3_spark=mock_s3_spark,\n",
    "        daterange=single_date,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\"\n",
    "    )\n",
    "    assert result_df is not None\n",
    "    mock_s3_spark.read.parquet.assert_called_once_with(\n",
    "        \"s3a://fake-bucket/fake-prefixdate_label=2021-01-01/\"\n",
    "    )\n",
    "\n",
    "def test__create_df_daterange_range_of_dates(spark_session, mocker):\n",
    "    from s3 import _create_df_daterange\n",
    "    mocker.patch(\"s3.get_text_before_date\", return_value=\"date_label=\")\n",
    "\n",
    "    mock_s3_spark = MagicMock()\n",
    "    df_stub = spark_session.createDataFrame([(\"test\",)], [\"col1\"])\n",
    "    mock_s3_spark.read.parquet.return_value = df_stub\n",
    "\n",
    "    date_range = [datetime(2021, 1, 1), datetime(2021, 1, 3)]\n",
    "    result_df = _create_df_daterange(\n",
    "        s3_spark=mock_s3_spark,\n",
    "        daterange=date_range,\n",
    "        bucket=\"fake-bucket\",\n",
    "        prefix=\"fake-prefix\"\n",
    "    )\n",
    "    assert result_df is not None\n",
    "\n",
    "    calls = mock_s3_spark.read.parquet.call_args_list\n",
    "    called_paths = [str(c[0][0]) for c in calls]\n",
    "    assert \"s3a://fake-bucket/fake-prefixdate_label=2021-01-01/\" in called_paths\n",
    "    assert \"s3a://fake-bucket/fake-prefixdate_label=2021-01-02/\" in called_paths\n",
    "    assert \"s3a://fake-bucket/fake-prefixdate_label=2021-01-03/\" in called_paths\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "#                   Integration Tests (Actual S3 Access)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_read_from_s3_spark_integration(mock_s3_spark):\n",
    "    \"\"\"\n",
    "    Example integration test that tries reading actual data from S3.\n",
    "    Requires valid AWS creds & real S3 data. \n",
    "    This test is marked as 'integration' so you can skip it in normal runs:\n",
    "      `pytest -m \"not integration\"`\n",
    "    \"\"\"\n",
    "    # Adjust these to point to a real S3 bucket/prefix that you have permission to read.\n",
    "    bucket = os.environ.get(\"TEST_S3_BUCKET\", \"my-real-bucket\")\n",
    "    prefix = os.environ.get(\"TEST_S3_PREFIX\", \"my-data/\")\n",
    "    start_date = datetime(2021, 1, 1)\n",
    "    end_date = datetime(2021, 1, 1)\n",
    "\n",
    "    df = read_from_s3_spark(\n",
    "        s3_spark=mock_s3_spark,  # This is either a real spark session configured for S3\n",
    "        bucket=bucket,\n",
    "        prefix=prefix,\n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        select=None,\n",
    "        filters=None,\n",
    "        parse_column=None\n",
    "    )\n",
    "\n",
    "    # If data exists for that date, we should be able to read some rows.\n",
    "    # Adjust logic depending on your real data. \n",
    "    # For example, you might expect non-empty data, or at least no error.\n",
    "    # If you expect real data, you can assert df.count() > 0\n",
    "    # If the prefix often might be empty, you can just ensure no exception was raised:\n",
    "    assert df is not None, \"Should successfully return a DataFrame from S3\"\n",
    "    # Optional: check columns or row counts\n",
    "    # assert df.count() > 0\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_read_from_s3_spark_nonexistent_bucket(mock_s3_spark):\n",
    "    \"\"\"\n",
    "    Negative test: reading from a bucket that does not exist \n",
    "    should raise an exception (AnalysisException or similar).\n",
    "    \"\"\"\n",
    "    with pytest.raises((AnalysisException, Exception)) as excinfo:\n",
    "        read_from_s3_spark(\n",
    "            s3_spark=mock_s3_spark,\n",
    "            bucket=\"bucket-that-likely-does-not-exist-1234\",\n",
    "            prefix=\"some-prefix/\",\n",
    "            start_date=datetime(2021, 1, 1),\n",
    "            end_date=datetime(2021, 1, 1)\n",
    "        )\n",
    "    # Some text in the exception message might indicate the bucket doesn't exist\n",
    "    # Adjust according to your environment (Spark might produce different messages).\n",
    "    assert \"does not exist\" in str(excinfo.value).lower() or \\\n",
    "           \"statuscode=404\" in str(excinfo.value).lower() or \\\n",
    "           \"not found\" in str(excinfo.value).lower()\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_read_from_s3_spark_nonexistent_prefix(mock_s3_spark):\n",
    "    \"\"\"\n",
    "    Negative test: reading from an empty/non-existent prefix \n",
    "    in an existing bucket should raise an error or return empty. \n",
    "    This may depend on your Spark/Hadoop version.\n",
    "    \"\"\"\n",
    "    bucket = os.environ.get(\"TEST_S3_BUCKET\", \"my-real-bucket\")\n",
    "    prefix = \"totally-made-up/prefix/\"\n",
    "\n",
    "    # If the prefix definitely doesn't exist, Spark often raises an AnalysisException.\n",
    "    # Some versions might allow an empty DataFrame to return if the directory is just empty. \n",
    "    # Adjust your logic accordingly. \n",
    "    with pytest.raises((AnalysisException, Exception)) as excinfo:\n",
    "        read_from_s3_spark(\n",
    "            s3_spark=mock_s3_spark,\n",
    "            bucket=bucket,\n",
    "            prefix=prefix,\n",
    "            start_date=datetime(2021, 1, 1),\n",
    "            end_date=datetime(2021, 1, 1),\n",
    "        )\n",
    "    # Example checks\n",
    "    err_str = str(excinfo.value).lower()\n",
    "    assert \"path does not exist\" in err_str or \"not found\" in err_str or \"statuscode=404\" in err_str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation and Key Points\n",
    "Unit Tests (Mocked)\n",
    "\n",
    "These tests (test_read_from_s3_spark_no_select_no_filters_no_parse, test_read_from_s3_spark_with_select, etc.) mock out the calls to S3 and only validate the internal logic of read_from_s3_spark.\n",
    "They prevent real AWS calls, which keeps tests fast and consistent.\n",
    "Integration Tests (Real S3)\n",
    "\n",
    "Marked with @pytest.mark.integration. By default, you could run them with pytest -m integration or skip them if you run pytest -m \"not integration\".\n",
    "They use the mock_s3_spark fixture, which could be a real SparkSession configured with your actual S3 endpoint and credentials.\n",
    "You set environment variables like TEST_S3_BUCKET and TEST_S3_PREFIX to control which real bucket/prefix you want to test.\n",
    "The test reads real data from S3. If the data is missing, it might raise an exception or produce an empty DataFrame, depending on your environment and Spark version.\n",
    "Edge Cases / Negative Tests\n",
    "\n",
    "test_read_from_s3_spark_nonexistent_bucket() tries to read from a clearly bogus bucket, expecting an exception.\n",
    "test_read_from_s3_spark_nonexistent_prefix() tries a valid bucket with a non-existent folder/prefix, often resulting in an error (e.g., AnalysisException) or an empty DataFrame. This can vary by Spark version; adjust the test to your environment.\n",
    "Running Tests\n",
    "\n",
    "Install dependencies: pytest, pytest-mock, pyspark, etc.\n",
    "Set environment variables for AWS credentials (e.g., AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) and the S3 endpoint if you’re not using AWS.\n",
    "Run tests:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest test_s3_read.py\n",
    "or\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest -m integration  # Only integration tests\n",
    "pytest -m \"not integration\"  # Only unit tests\n",
    "This layout gives you both confidence that your code behaves as expected (unit tests) and verifies that the pipeline works with an actual S3 environment (integration tests). Adjust bucket names, prefixes, and environment variable usage to fit your setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of how you might organize unit tests and integration tests for db.py using pytest. This example assumes:\n",
    "\n",
    "You have the DB class in db.py (as you showed) that can connect to either Elasticsearch (es) or S3 (s3) or read from a file.\n",
    "You have a working read_from_s3_spark function in s3.py.\n",
    "You want to test both how DB interacts with read_from_s3_spark (mocked unit tests) and run an integration test that actually hits a real S3 environment.\n",
    "Feel free to mix these tests into the same file (e.g., test_db.py) or break them into multiple files. The example below uses a single file for clarity.\n",
    "\n",
    "Example: test_db.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "from unittest.mock import MagicMock, patch\n",
    "from datetime import datetime\n",
    "\n",
    "# Import your DB class and the read_from_s3_spark function\n",
    "from db import DB\n",
    "from s3 import read_from_s3_spark\n",
    "\n",
    "##########################\n",
    "#      Pytest Fixtures\n",
    "##########################\n",
    "\n",
    "@pytest.fixture\n",
    "def s3_config():\n",
    "    \"\"\"\n",
    "    Returns a sample config dict pointing to S3.\n",
    "    Adjust 'endpoint', 'bucket', etc., to fit your environment or tests.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"loc\": \"s3\",\n",
    "        \"s3\": {\n",
    "            \"endpoint\": \"fake-endpoint\",\n",
    "            \"bucket\": \"fake-bucket\",\n",
    "            \"prefix\": \"some-prefix/\",\n",
    "            \"filters\": {\"key\": \"value\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def es_config():\n",
    "    \"\"\"\n",
    "    Returns a sample config dict pointing to an ES instance.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"loc\": \"es\",\n",
    "        \"es\": {\n",
    "            \"endpoint\": \"fake-es-host\",\n",
    "            \"index\": \"my-index\",\n",
    "            \"filters\": {\"es_key\": \"es_val\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def file_config():\n",
    "    \"\"\"\n",
    "    Returns a sample config dict pointing to a local file.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"loc\": \"file\",\n",
    "        \"file\": {\n",
    "            \"path\": \"/tmp/fake_file.txt\",\n",
    "        },\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def infra_config():\n",
    "    \"\"\"\n",
    "    Returns a sample infra_config that DB looks at for environment details.\n",
    "    Typically you have keys like 'prod_s3', 'prod_es', 'prod_spark', etc.\n",
    "    Adjust or remove as needed.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"prod_es\": [\"fake-es-host\"],     # if host is in here, it’s considered \"prod\"\n",
    "        \"prod_s3\": [\"fake-endpoint\"],    # likewise for S3\n",
    "        \"prod_spark\": [\"spark://fake-spark-cluster:7077\"],\n",
    "    }\n",
    "\n",
    "##########################\n",
    "#        Unit Tests\n",
    "##########################\n",
    "\n",
    "def test_db_extract_config_s3(s3_config, infra_config):\n",
    "    \"\"\"\n",
    "    Test DB._extract_config() logic for S3 type.\n",
    "    \"\"\"\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "    assert db_obj.db_type == \"s3\"\n",
    "    assert db_obj.db_config == s3_config[\"s3\"]\n",
    "\n",
    "def test_db_extract_config_es(es_config, infra_config):\n",
    "    \"\"\"\n",
    "    Test DB._extract_config() logic for ES type.\n",
    "    \"\"\"\n",
    "    db_obj = DB(config=es_config, infra_config=infra_config)\n",
    "    assert db_obj.db_type == \"es\"\n",
    "    assert db_obj.db_config == es_config[\"es\"]\n",
    "\n",
    "def test_db_extract_config_file(file_config, infra_config):\n",
    "    \"\"\"\n",
    "    Test DB._extract_config() logic for file type.\n",
    "    \"\"\"\n",
    "    db_obj = DB(config=file_config, infra_config=infra_config)\n",
    "    assert db_obj.db_type == \"file\"\n",
    "    assert db_obj.db_config == file_config[\"file\"]\n",
    "\n",
    "def test_db_connect_s3_valid(s3_config, infra_config, mocker):\n",
    "    \"\"\"\n",
    "    Test DB.connect() for S3 type with valid config.\n",
    "    Mocks init_s3_spark so we don't create a real Spark session.\n",
    "    \"\"\"\n",
    "    # Arrange\n",
    "    mock_init_s3 = mocker.patch(\"db.init_s3_spark\", return_value=\"mocked_spark_session\")\n",
    "\n",
    "    # Act\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "    s3_session, err = db_obj.connect()\n",
    "\n",
    "    # Assert\n",
    "    mock_init_s3.assert_called_once_with(\n",
    "        \"fake-endpoint\",                # from config\n",
    "        os.getenv(\"S3_ACCESS_KEY\"),     # from environment\n",
    "        os.getenv(\"S3_SECRET_KEY\"),     # from environment\n",
    "        mocker.ANY,                     # spark_home\n",
    "        \"spark://fake-spark-cluster:7077\",  # from infra_config['prod_spark']\n",
    "        os.getenv(\"CA_CERTS\"),          # from environment\n",
    "    )\n",
    "    assert s3_session == \"mocked_spark_session\"\n",
    "    assert err is None\n",
    "\n",
    "def test_db_connect_s3_invalid_config(s3_config, infra_config):\n",
    "    \"\"\"\n",
    "    Test DB.connect() for S3 type when config is missing an endpoint or bucket.\n",
    "    The code returns (None, \"INVALID_S3_CONFIG\") in that case.\n",
    "    \"\"\"\n",
    "    # Break the config to remove 'bucket'\n",
    "    del s3_config[\"s3\"][\"bucket\"]\n",
    "\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "    s3_session, err = db_obj.connect()\n",
    "\n",
    "    assert s3_session is None\n",
    "    assert err == \"INVALID_S3_CONFIG\"\n",
    "\n",
    "def test_db_connect_es_valid(es_config, infra_config, mocker):\n",
    "    \"\"\"\n",
    "    Test DB.connect() for ES with mocking of init_es.\n",
    "    \"\"\"\n",
    "    mock_init_es = mocker.patch(\"db.init_es\", return_value=(\"mocked_es_session\", None))\n",
    "\n",
    "    db_obj = DB(config=es_config, infra_config=infra_config)\n",
    "    es_session, err = db_obj.connect()\n",
    "\n",
    "    assert es_session == \"mocked_es_session\"\n",
    "    assert err is None\n",
    "    # The code calls db_obj._get_infra_info(host) and eventually init_es\n",
    "    mock_init_es.assert_called_once()\n",
    "\n",
    "def test_db_read_data_s3_unit(s3_config, infra_config, mocker):\n",
    "    \"\"\"\n",
    "    Unit test of DB.read_data() for S3, mocking read_from_s3_spark.\n",
    "    \"\"\"\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "    db_obj.s3 = \"fake_spark_session\"  # pretend we already connected\n",
    "\n",
    "    mock_read_s3 = mocker.patch(\"db.read_from_s3_spark\", return_value=\"some_dataframe\")\n",
    "\n",
    "    start_date = datetime(2021, 1, 1)\n",
    "    end_date = datetime(2021, 1, 2)\n",
    "\n",
    "    result = db_obj.read_data(start_date, end_date, parse_column=\"some_col\")\n",
    "\n",
    "    # read_from_s3_spark should be called once with the matching arguments:\n",
    "    mock_read_s3.assert_called_once_with(\n",
    "        \"fake_spark_session\",\n",
    "        \"fake-bucket\",\n",
    "        \"some-prefix/\",\n",
    "        start_date,\n",
    "        end_date,\n",
    "        select=[\"*\"],\n",
    "        filters={\"key\": \"value\"},\n",
    "        parse_column=\"some_col\",\n",
    "    )\n",
    "    assert result == \"some_dataframe\"\n",
    "\n",
    "def test_db_disconnect_s3(s3_config, infra_config, mocker):\n",
    "    \"\"\"\n",
    "    Test DB.disconnect() for S3. Ensure it calls s3.stop() \n",
    "    if self.db_type == 's3'.\n",
    "    \"\"\"\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "    db_obj.s3 = MagicMock()\n",
    "    db_obj.db_type = \"s3\"\n",
    "\n",
    "    db_obj.disconnect()\n",
    "    db_obj.s3.stop.assert_called_once()\n",
    "\n",
    "##########################\n",
    "#  Integration Test (S3)\n",
    "##########################\n",
    "\n",
    "@pytest.mark.integration\n",
    "def test_db_read_data_s3_integration(s3_config, infra_config):\n",
    "    \"\"\"\n",
    "    Integration test that tries to connect to a real S3 + Spark \n",
    "    and read actual data. Requires valid AWS credentials and \n",
    "    a real S3 endpoint if 'fake-endpoint' is replaced with a real one.\n",
    "\n",
    "    Example usage:\n",
    "      pytest -m integration\n",
    "    \n",
    "    Adjust environment variables or s3_config so that:\n",
    "     - s3_config['s3']['endpoint'] is a real endpoint \n",
    "     - s3_config['s3']['bucket'] is a real bucket \n",
    "     - s3_config['s3']['prefix'] is valid or might contain data \n",
    "     - infra_config['prod_s3'] includes that real endpoint\n",
    "     - infra_config['prod_spark'] is a real Spark master \n",
    "     - AWS creds are set in environment\n",
    "    \"\"\"\n",
    "    # Overwrite s3_config with real data for integration\n",
    "    s3_config[\"s3\"][\"endpoint\"] = os.environ.get(\"REAL_S3_ENDPOINT\", \"your-s3-endpoint.amazonaws.com\")\n",
    "    s3_config[\"s3\"][\"bucket\"]   = os.environ.get(\"REAL_S3_BUCKET\", \"your-bucket\")\n",
    "    s3_config[\"s3\"][\"prefix\"]   = os.environ.get(\"REAL_S3_PREFIX\", \"some/path/\")\n",
    "    \n",
    "    # Mark it as \"prod\" so the logic picks up environment-based credentials:\n",
    "    infra_config[\"prod_s3\"] = [s3_config[\"s3\"][\"endpoint\"]]\n",
    "\n",
    "    # Create the DB object\n",
    "    db_obj = DB(config=s3_config, infra_config=infra_config)\n",
    "\n",
    "    # Connect\n",
    "    s3_session, err = db_obj.connect()\n",
    "    assert s3_session is not None, f\"Expected a valid S3 Spark session, got err={err}\"\n",
    "\n",
    "    # Attempt to read data\n",
    "    start_date = datetime(2023, 1, 1)\n",
    "    end_date = datetime(2023, 1, 1)\n",
    "\n",
    "    df = db_obj.read_data(\n",
    "        start_date=start_date, \n",
    "        end_date=end_date, \n",
    "        fields=[\"*\"], \n",
    "        parse_column=None\n",
    "    )\n",
    "\n",
    "    # Check if the DataFrame is valid\n",
    "    # Depending on your data, you might assert df.count() > 0, or \n",
    "    # just ensure it doesn't fail.\n",
    "    assert df is not None, \"Expected a Spark DataFrame from S3.\"\n",
    "    # e.g., maybe check schema or rowcount:\n",
    "    # print(df.schema)\n",
    "    # print(df.show())\n",
    "\n",
    "    # Cleanup\n",
    "    db_obj.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "Pytest Fixtures\n",
    "\n",
    "s3_config, es_config, file_config: Provide test configs for different DB types.\n",
    "infra_config: Mock infrastructure info that the DB class uses to determine if an endpoint is “prod” or not.\n",
    "Unit Tests\n",
    "\n",
    "test_db_extract_config_*: Verifies _extract_config() sets db_type and db_config properly based on config.\n",
    "test_db_connect_s3_valid: Mocks init_s3_spark so we don’t spin up a real Spark session. Checks that the DB.connect() method sets self.s3 and returns (s3_session, None).\n",
    "test_db_connect_s3_invalid_config: If endpoint or bucket is missing, code returns (None, \"INVALID_S3_CONFIG\").\n",
    "test_db_connect_es_valid: Mocks init_es (for Elasticsearch) so it doesn’t connect to a real ES cluster.\n",
    "test_db_read_data_s3_unit: Mocks read_from_s3_spark to ensure the DB.read_data() calls it with correct arguments.\n",
    "test_db_disconnect_s3: Ensures db_obj.s3.stop() is called when disconnect() is called on an S3-based DB.\n",
    "Integration Test\n",
    "\n",
    "test_db_read_data_s3_integration:\n",
    "Marked with @pytest.mark.integration so you can selectively run it with pytest -m integration.\n",
    "Adjust the config to real S3 credentials and endpoints at runtime.\n",
    "Calls connect() on DB, ensures no error, and tries read_data().\n",
    "Optionally checks row count, schema, or whether the call completed without error.\n",
    "Finally calls disconnect().\n",
    "Running the Tests\n",
    "\n",
    "Make sure you have pytest (and optionally pytest-mock) installed:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pip install pytest pytest-mock\n",
    "Place this file (test_db.py) in your test directory or next to db.py.\n",
    "Run the tests:\n",
    "All tests:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest test_db.py\n",
    "Just integration:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest -m integration\n",
    "All except integration:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest -m \"not integration\"\n",
    "Environment Variables\n",
    "\n",
    "For real integration, you’ll need AWS credentials for S3 in the environment (e.g., AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY) or whichever environment variables your code references (S3_ACCESS_KEY, S3_SECRET_KEY).\n",
    "You can override them with your own environment variables if needed.\n",
    "With this structure, you’ll have comprehensive coverage:\n",
    "\n",
    "Unit tests verify your DB class logic and how it calls read_from_s3_spark.\n",
    "Integration tests confirm it actually works with live S3.\n",
    "Feel free to refine or expand these tests based on your specific environment, naming conventions, or additional requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a Pytest-based unit test module for MetricsSpark.py that follows the same parametrized style and structure as the code snippet you provided. It creates synthetic data, runs the vdr, tdr, and tfpr methods, and compares the results against manually computed expectations. You can place this in a file like test_metrics_spark.py alongside your MetricsSpark.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from src.analysis.metrics_spark import MetricsSpark\n",
    "\n",
    "#\n",
    "# --------------- Pytest Fixture for SparkSession ---------------\n",
    "#\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def spark_session():\n",
    "    \"\"\"\n",
    "    Creates a local Spark session once per test session.\n",
    "    \"\"\"\n",
    "    return SparkSession.builder.appName(\"Test\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "#\n",
    "# --------------- Helpers for Generating & Computing ---------------\n",
    "#\n",
    "\n",
    "def _gen_random_values(val_range=10, n=1000, offset=0, **kwargs):\n",
    "    \"\"\"\n",
    "    Generates random score values, binary labels, and amounts for testing.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    values = np.random.uniform(-val_range, val_range, n) + offset\n",
    "    labels = np.random.choice([0, 1], size=n)\n",
    "    amt_values = np.random.uniform(1, 100, n)  # amounts between 1 and 100\n",
    "    return {\"values\": values, \"labels\": labels, \"amt_values\": amt_values}\n",
    "\n",
    "def _cal_vdr(df, field, threshold, amt_field):\n",
    "    \"\"\"\n",
    "    Manually compute \"Value Detection Rate\":\n",
    "      sum of amounts captured by the model / total fraudulent amount\n",
    "    \"\"\"\n",
    "    tp_value = 0\n",
    "    total_value = 0\n",
    "\n",
    "    for row in df.collect():\n",
    "        amount = row[amt_field]\n",
    "        label = row[field + \"_label\"]\n",
    "        score = row[field]\n",
    "\n",
    "        if label == 1:           # ground-truth is fraud\n",
    "            total_value += amount\n",
    "            if score > threshold:\n",
    "                tp_value += amount\n",
    "\n",
    "    return tp_value / total_value if total_value != 0 else None\n",
    "\n",
    "def _cal_tdr(df, field, threshold):\n",
    "    \"\"\"\n",
    "    Manually compute \"True Detection Rate\":\n",
    "      #fraud captured / #total fraud\n",
    "    \"\"\"\n",
    "    tp_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for row in df.collect():\n",
    "        label = row[field + \"_label\"]\n",
    "        score = row[field]\n",
    "\n",
    "        if label == 1:\n",
    "            total_count += 1\n",
    "            if score > threshold:\n",
    "                tp_count += 1\n",
    "\n",
    "    return tp_count / total_count if total_count != 0 else None\n",
    "\n",
    "def _cal_tfpr(df, field, threshold):\n",
    "    \"\"\"\n",
    "    Manually compute ratio of false positives to true positives:\n",
    "      fp / tp\n",
    "    \"\"\"\n",
    "    tp_count = 0\n",
    "    fp_count = 0\n",
    "\n",
    "    for row in df.collect():\n",
    "        label = row[field + \"_label\"]\n",
    "        score = row[field]\n",
    "\n",
    "        if score > threshold:\n",
    "            if label == 1:\n",
    "                tp_count += 1\n",
    "            else:\n",
    "                fp_count += 1\n",
    "\n",
    "    return fp_count / tp_count if tp_count != 0 else None\n",
    "\n",
    "def _gen_case(case_name, metric, func, args, spark_session):\n",
    "    \"\"\"\n",
    "    Creates a Spark DataFrame with 'score', 'score_label', and 'amount'\n",
    "    columns, then computes an expected result by calling the manual\n",
    "    calculators above. Returns a dictionary of all relevant info.\n",
    "    \"\"\"\n",
    "    # 1) Generate or retrieve synthetic inputs\n",
    "    inputs = func(**args)\n",
    "    values = inputs[\"values\"]\n",
    "    labels = inputs[\"labels\"]\n",
    "    amt_values = inputs[\"amt_values\"]\n",
    "\n",
    "    # 2) Build a Spark DF\n",
    "    df = spark_session.createDataFrame(\n",
    "        [\n",
    "            Row(score=val, score_label=label, amount=amt)\n",
    "            for val, label, amt in zip(values, labels, amt_values)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    field = \"score\"\n",
    "    threshold = args.get(\"threshold\", 0.5)\n",
    "    amt_field = \"amount\"\n",
    "\n",
    "    # 3) Compute the 'expected' result manually\n",
    "    if metric == \"vdr\":\n",
    "        exp_res = _cal_vdr(df, field, threshold, amt_field)\n",
    "    elif metric == \"tdr\":\n",
    "        exp_res = _cal_tdr(df, field, threshold)\n",
    "    elif metric == \"tfpr\":\n",
    "        exp_res = _cal_tfpr(df, field, threshold)\n",
    "    else:\n",
    "        exp_res = None\n",
    "\n",
    "    return {\n",
    "        \"case\": case_name,\n",
    "        \"inputs\": {\n",
    "            \"df\": df,\n",
    "            \"field\": field,\n",
    "            \"threshold\": threshold,\n",
    "            \"amt_field\": amt_field,\n",
    "        },\n",
    "        \"expected\": {\"res\": exp_res, \"tag\": None},\n",
    "    }\n",
    "\n",
    "def _gen_data(metric, spark_session):\n",
    "    \"\"\"\n",
    "    Creates a list of test cases (dictionaries) for a given metric.\n",
    "    Each entry includes the DataFrame inputs, the expected result, etc.\n",
    "    \"\"\"\n",
    "    data = [\n",
    "        _gen_case(\"all positive values\", metric, _gen_random_values, {}, spark_session),\n",
    "        _gen_case(\n",
    "            \"no fraud amount\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"val_range\": -10},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"mixed values\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"offset\": 0.5},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"large values\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"val_range\": 1e7, \"offset\": 0.5},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"small values\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"val_range\": 1e-9, \"offset\": 0.5},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"empty data\",\n",
    "            metric,\n",
    "            lambda **kw: {\"values\": [], \"labels\": [], \"amt_values\": []},\n",
    "            {},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"single class\",\n",
    "            metric,\n",
    "            lambda **kw: {\n",
    "                \"values\": [1, 1, 1, 1],\n",
    "                \"labels\": [1, 1, 1, 1],\n",
    "                \"amt_values\": [10, 10, 10, 10],\n",
    "            },\n",
    "            {},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"all zero values\",\n",
    "            metric,\n",
    "            lambda **kw: {\n",
    "                \"values\": [0, 0, 0, 0],\n",
    "                \"labels\": [0, 0, 0, 0],\n",
    "                \"amt_values\": [0, 0, 0, 0],\n",
    "            },\n",
    "            {},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"all one values\",\n",
    "            metric,\n",
    "            lambda **kw: {\n",
    "                \"values\": [1, 1, 1, 1],\n",
    "                \"labels\": [1, 1, 1, 1],\n",
    "                \"amt_values\": [1, 1, 1, 1],\n",
    "            },\n",
    "            {},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"high threshold\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"threshold\": 100},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"low threshold\",\n",
    "            metric,\n",
    "            _gen_random_values,\n",
    "            {\"threshold\": -100},\n",
    "            spark_session,\n",
    "        ),\n",
    "        _gen_case(\n",
    "            \"nan values\",\n",
    "            metric,\n",
    "            lambda **kw: {\n",
    "                \"values\": [1, 2, np.nan],\n",
    "                \"labels\": [1, 0, 1],\n",
    "                \"amt_values\": [10, 20, 30],\n",
    "            },\n",
    "            {},\n",
    "            spark_session,\n",
    "        ),\n",
    "    ]\n",
    "    return data\n",
    "\n",
    "#\n",
    "# --------------- Parametrized Tests for Each Metric ---------------\n",
    "#\n",
    "\n",
    "@pytest.fixture()\n",
    "def gen_data(request):\n",
    "    return request.param\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gen_data\",\n",
    "    argvalues=_gen_data(\"vdr\", spark_session()),\n",
    "    indirect=True,\n",
    "    ids=[t[\"case\"] for t in _gen_data(\"vdr\", spark_session())],\n",
    ")\n",
    "def test_vdr(gen_data, spark_session):\n",
    "    \"\"\"\n",
    "    Test the vdr() method of MetricsSpark with various synthetic data sets.\n",
    "    \"\"\"\n",
    "    df = gen_data[\"inputs\"][\"df\"]\n",
    "    field = gen_data[\"inputs\"][\"field\"]\n",
    "    threshold = gen_data[\"inputs\"][\"threshold\"]\n",
    "    amt_field = gen_data[\"inputs\"][\"amt_field\"]\n",
    "\n",
    "    res, tag = MetricsSpark.vdr(df, field, threshold, amt_field)\n",
    "\n",
    "    # Compare to our pre-computed expected result\n",
    "    assert res == gen_data[\"expected\"][\"res\"], f\"Case: {gen_data['case']}\"\n",
    "    if \"tag\" in gen_data[\"expected\"]:\n",
    "        assert tag == gen_data[\"expected\"][\"tag\"]\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gen_data\",\n",
    "    argvalues=_gen_data(\"tdr\", spark_session()),\n",
    "    indirect=True,\n",
    "    ids=[t[\"case\"] for t in _gen_data(\"tdr\", spark_session())],\n",
    ")\n",
    "def test_tdr(gen_data, spark_session):\n",
    "    \"\"\"\n",
    "    Test the tdr() method of MetricsSpark.\n",
    "    \"\"\"\n",
    "    df = gen_data[\"inputs\"][\"df\"]\n",
    "    field = gen_data[\"inputs\"][\"field\"]\n",
    "    threshold = gen_data[\"inputs\"][\"threshold\"]\n",
    "\n",
    "    res, tag = MetricsSpark.tdr(df, field, threshold)\n",
    "\n",
    "    assert res == gen_data[\"expected\"][\"res\"], f\"Case: {gen_data['case']}\"\n",
    "    if \"tag\" in gen_data[\"expected\"]:\n",
    "        assert tag == gen_data[\"expected\"][\"tag\"]\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"gen_data\",\n",
    "    argvalues=_gen_data(\"tfpr\", spark_session()),\n",
    "    indirect=True,\n",
    "    ids=[t[\"case\"] for t in _gen_data(\"tfpr\", spark_session())],\n",
    ")\n",
    "def test_tfpr(gen_data, spark_session):\n",
    "    \"\"\"\n",
    "    Test the tfpr() method of MetricsSpark.\n",
    "    \"\"\"\n",
    "    df = gen_data[\"inputs\"][\"df\"]\n",
    "    field = gen_data[\"inputs\"][\"field\"]\n",
    "    threshold = gen_data[\"inputs\"][\"threshold\"]\n",
    "\n",
    "    res, tag = MetricsSpark.tfpr(df, field, threshold)\n",
    "\n",
    "    assert res == gen_data[\"expected\"][\"res\"], f\"Case: {gen_data['case']}\"\n",
    "    if \"tag\" in gen_data[\"expected\"]:\n",
    "        assert tag == gen_data[\"expected\"][\"tag\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How This Works\n",
    "spark_session Fixture\n",
    "\n",
    "Creates a local Spark session once per module.\n",
    "Synthetic Data Functions\n",
    "\n",
    "_gen_random_values(): produces random scores, binary labels, and amounts.\n",
    "_cal_*() functions: manually compute the expected metric (vdr, tdr, tfpr) by iterating over the DataFrame.\n",
    "_gen_case()\n",
    "\n",
    "Uses the above functions to build a small DataFrame of (score, score_label, amount).\n",
    "Determines what the correct \"expected\" result should be by calling the manual _cal_*() counterpart.\n",
    "_gen_data()\n",
    "\n",
    "Constructs a list of distinct test cases (e.g., “large values,” “no fraud amount,” “nan values,” etc.).\n",
    "Each entry is a dictionary containing the test DataFrame, threshold, etc., and the expected metric result.\n",
    "Parametrized Tests\n",
    "\n",
    "For each metric (vdr, tdr, tfpr), we parametrize over the entire list of test cases returned by _gen_data().\n",
    "test_vdr, test_tdr, test_tfpr each call the corresponding MetricsSpark method, then assert that the returned (res, tag) matches the pre-computed expected[\"res\"] (and expected[\"tag\"] if present).\n",
    "Running the Tests\n",
    "\n",
    "Make sure pytest is installed:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pip install pytest\n",
    "From your project directory (where test_metrics_spark.py is located), run:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest test_metrics_spark.py\n",
    "or simply\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest\n",
    "if the file follows standard test discovery rules.\n",
    "This setup thoroughly checks each metric function (vdr, tdr, tfpr) against a variety of data scenarios. It uses a consistent approach to fixture-based testing, as shown in your example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST Gound Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example pytest test suite for the new SPM flow that covers both non–ground-truth metrics and ground-truth metrics. The tests demonstrate how you might:\n",
    "\n",
    "Mock out calls to DB, Analysis, etc.\n",
    "Provide dummy data for both ground-truth and non–ground-truth use cases.\n",
    "Follow a similar pattern to the metrics_spark.py tests (where we generate or mock data) and the db.py/s3_db tests (where we often mock out the underlying database calls).\n",
    "Note: This is a sample test showing the structure and approach. You will likely need to adapt it to your actual codebase – for instance, how your SPM and Analysis classes are imported, or where your test configs live. Also, since SPM depends on reading config files, some portions are mocked in the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pytest\n",
    "import time\n",
    "from unittest.mock import MagicMock, patch\n",
    "from datetime import datetime\n",
    "\n",
    "# Example imports from your codebase:\n",
    "# from my_project.spm.spm_flow import SPM  # Adjust to your actual import path\n",
    "# from my_project.database.db import DB\n",
    "# from my_project.analysis.analysis_new import Analysis\n",
    "\n",
    "#\n",
    "# --------------- Fixtures & Helpers ---------------\n",
    "#\n",
    "\n",
    "@pytest.fixture\n",
    "def spm_test_dir(tmp_path):\n",
    "    \"\"\"\n",
    "    A pytest fixture that creates a temporary directory for test task files.\n",
    "    We can place mock config files there if needed.\n",
    "    \"\"\"\n",
    "    return tmp_path\n",
    "\n",
    "@pytest.fixture\n",
    "def mock_s3_db(mocker):\n",
    "    \"\"\"\n",
    "    Example fixture that returns a mock DB or s3 DB object.\n",
    "    We patch DB so that calls to connect / read_data are replaced by mocks.\n",
    "    \"\"\"\n",
    "    mock_connect = mocker.patch(\"my_project.database.db.DB.connect\", return_value=(None, None))\n",
    "    mock_read_data = mocker.patch(\"my_project.database.db.DB.read_data\", return_value=[])\n",
    "    mock_disconnect = mocker.patch(\"my_project.database.db.DB.disconnect\")\n",
    "    return {\n",
    "        \"connect_patch\": mock_connect,\n",
    "        \"read_data_patch\": mock_read_data,\n",
    "        \"disconnect_patch\": mock_disconnect\n",
    "    }\n",
    "\n",
    "@pytest.fixture\n",
    "def spm_config_ground_truth(spm_test_dir):\n",
    "    \"\"\"\n",
    "    Creates a minimal config file for a ground-truth metric scenario.\n",
    "    Puts the file in spm_test_dir/spm_configs/<task>.yml \n",
    "    so SPM can read it if needed.\n",
    "    \"\"\"\n",
    "    # Example config structure referencing ground-truth fields\n",
    "    config_content = \"\"\"\n",
    "model_name: \"test_model\"\n",
    "model_version: \"v1\"\n",
    "model_type: \"binary\"\n",
    "fields:\n",
    "  ground_truth_field: \n",
    "    type: \"gt\"\n",
    "    # ...\n",
    "  score_field:\n",
    "    type: \"non_gt\"\n",
    "data:\n",
    "  loc: \"s3\"\n",
    "  endpoint: \"fake-endpoint\"\n",
    "  bucket: \"fake-bucket\"\n",
    "gt_fields:\n",
    "  ground_truth:\n",
    "    backfill:\n",
    "      interval: \"1d\"\n",
    "      n: 2\n",
    "    id: [\"unique_id\"]\n",
    "\"\"\"\n",
    "    config_file = spm_test_dir / \"spm_configs\" / \"task_gt.yml\"\n",
    "    config_file.parent.mkdir(exist_ok=True)\n",
    "    config_file.write_text(config_content)\n",
    "    return config_file\n",
    "\n",
    "@pytest.fixture\n",
    "def spm_config_non_ground_truth(spm_test_dir):\n",
    "    \"\"\"\n",
    "    Creates a minimal config for a non–ground-truth metric scenario.\n",
    "    \"\"\"\n",
    "    config_content = \"\"\"\n",
    "model_name: \"test_model_nogt\"\n",
    "model_version: \"v2\"\n",
    "model_type: \"regression\"\n",
    "fields:\n",
    "  score_field:\n",
    "    type: \"non_gt\"\n",
    "data:\n",
    "  loc: \"s3\"\n",
    "  endpoint: \"fake-endpoint\"\n",
    "  bucket: \"fake-bucket\"\n",
    "\"\"\"\n",
    "    config_file = spm_test_dir / \"spm_configs\" / \"task_nogt.yml\"\n",
    "    config_file.parent.mkdir(exist_ok=True)\n",
    "    config_file.write_text(config_content)\n",
    "    return config_file\n",
    "\n",
    "@pytest.fixture\n",
    "def spm_tasks_file(spm_test_dir):\n",
    "    \"\"\"\n",
    "    Creates a minimal tasks.json file referencing tasks used by SPM.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    tasks = [\"task_gt\", \"task_nogt\"]\n",
    "    tasks_file = spm_test_dir / \"tasks.json\"\n",
    "    with open(tasks_file, \"w\") as f:\n",
    "        json.dump(tasks, f)\n",
    "    return tasks_file\n",
    "\n",
    "@pytest.fixture\n",
    "def spm_hist_tasks_file(spm_test_dir):\n",
    "    \"\"\"\n",
    "    Creates a hist_tasks.json if needed. Could be empty or reference tasks.\n",
    "    \"\"\"\n",
    "    import json\n",
    "    hist_tasks_file = spm_test_dir / \"hist_tasks.json\"\n",
    "    with open(hist_tasks_file, \"w\") as f:\n",
    "        # Example: we can pretend there's no historical tasks\n",
    "        json.dump({}, f)\n",
    "    return hist_tasks_file\n",
    "\n",
    "#\n",
    "# --------------- Example Tests ---------------\n",
    "#\n",
    "\n",
    "def test_spm_ground_truth_metrics(\n",
    "    mocker,\n",
    "    spm_test_dir,\n",
    "    spm_tasks_file,\n",
    "    spm_hist_tasks_file,\n",
    "    spm_config_ground_truth,\n",
    "    mock_s3_db\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests an SPM scenario with ground-truth metrics.\n",
    "    - We create an SPM object pointing to the tasks file (which includes \"task_gt\").\n",
    "    - We patch or mock certain calls (DB, Analysis, etc.).\n",
    "    - We ensure SPM.process_tasks() runs with no error, calls ground-truth calculations, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mock environment & ES indexing\n",
    "    mocker.patch.dict(os.environ, {\"USERNAME\": \"user\", \"PASSWORD\": \"pass\"})\n",
    "    mock_index_document = mocker.patch(\"my_project.database.es.index_document\", return_value=(\"mock_es_res\", None))\n",
    "\n",
    "    # Mock the Analysis class so we can confirm ground-truth metrics are called\n",
    "    mock_analysis = mocker.patch(\"my_project.analysis.analysis_new.Analysis\", autospec=True)\n",
    "    instance_analysis = mock_analysis.return_value\n",
    "    instance_analysis.cal_metrics.return_value = None\n",
    "    instance_analysis.get_result.return_value = {\n",
    "        \"some_metric\": 123,\n",
    "        \"some_other_metric\": 0.99\n",
    "    }\n",
    "\n",
    "    # Now create the SPM object\n",
    "    from my_project.spm.spm_flow import SPM  # or whichever path your SPM code is in\n",
    "    spm_obj = SPM(\n",
    "        task_filename=spm_tasks_file.name, \n",
    "        task_dir=str(spm_test_dir),\n",
    "        curr_time=datetime(2025, 1, 1),\n",
    "        window_size=7\n",
    "    )\n",
    "\n",
    "    # We run process_tasks. This should parse \"task_gt\" from tasks.json,\n",
    "    # load spm_configs/task_gt.yml, then run ground-truth logic.\n",
    "    spm_obj.process_tasks()\n",
    "\n",
    "    # Assertions/Verifications\n",
    "    # 1. Did we call DB.read_data for ground-truth data?\n",
    "    assert mock_s3_db[\"read_data_patch\"].call_count > 0, \"Expected to load data from DB\"\n",
    "\n",
    "    # 2. Analysis class used for metric calculations?\n",
    "    mock_analysis.assert_called_once()\n",
    "\n",
    "    # 3. Did we index results into ES?\n",
    "    mock_index_document.assert_called()\n",
    "\n",
    "def test_spm_non_ground_truth_metrics(\n",
    "    mocker,\n",
    "    spm_test_dir,\n",
    "    spm_tasks_file,\n",
    "    spm_hist_tasks_file,\n",
    "    spm_config_non_ground_truth,\n",
    "    mock_s3_db\n",
    "):\n",
    "    \"\"\"\n",
    "    Tests an SPM scenario with only non–ground-truth metrics.\n",
    "    This references \"task_nogt\" from tasks.json.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mock environment & ES indexing\n",
    "    mocker.patch.dict(os.environ, {\"USERNAME\": \"user\", \"PASSWORD\": \"pass\"})\n",
    "    mock_index_document = mocker.patch(\"my_project.database.es.index_document\", return_value=(\"mock_es_res\", None))\n",
    "\n",
    "    # Mock the Analysis class\n",
    "    mock_analysis = mocker.patch(\"my_project.analysis.analysis_new.Analysis\", autospec=True)\n",
    "    instance_analysis = mock_analysis.return_value\n",
    "    instance_analysis.cal_metrics.return_value = None\n",
    "    instance_analysis.get_result.return_value = {\n",
    "        \"non_gt_metric\": 42\n",
    "    }\n",
    "\n",
    "    # Create the SPM object\n",
    "    from my_project.spm.spm_flow import SPM\n",
    "    spm_obj = SPM(\n",
    "        task_filename=spm_tasks_file.name, \n",
    "        task_dir=str(spm_test_dir),\n",
    "        curr_time=datetime(2025, 1, 10),\n",
    "        window_size=14\n",
    "    )\n",
    "\n",
    "    # Run the tasks -> \"task_nogt\"\n",
    "    spm_obj.process_tasks()\n",
    "\n",
    "    # Assertions\n",
    "    assert mock_s3_db[\"read_data_patch\"].call_count > 0, \"Should read data for non-gt calculations\"\n",
    "    mock_analysis.assert_called_once()\n",
    "    mock_index_document.assert_called()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation\n",
    "Fixtures\n",
    "\n",
    "spm_test_dir: Creates a temporary directory for storing config files (task_nogt.yml, task_gt.yml, etc.).\n",
    "mock_s3_db: Patches DB.connect, DB.read_data, DB.disconnect so calls to your database are mocked.\n",
    "spm_config_ground_truth and spm_config_non_ground_truth: Writes out minimal .yml config files for each scenario (ground-truth vs. non–ground-truth).\n",
    "spm_tasks_file: Writes a tasks.json containing tasks referencing those config files.\n",
    "Ground-Truth vs. Non–Ground-Truth\n",
    "\n",
    "We have two separate tests:\n",
    "test_spm_ground_truth_metrics references task_gt, which loads spm_configs/task_gt.yml containing gt_fields. We expect the code to run ground-truth metric logic.\n",
    "test_spm_non_ground_truth_metrics references task_nogt, which only has non–ground-truth fields.\n",
    "Mocking\n",
    "\n",
    "We mock index_document from my_project.database.es to avoid real ES indexing.\n",
    "We mock Analysis from my_project.analysis.analysis_new to confirm it gets instantiated and check the metrics results flow.\n",
    "mock_s3_db ensures SPM’s calls to DB.connect(), read_data(), disconnect() do not hit a real database.\n",
    "Assertions\n",
    "\n",
    "We verify that read_data() was called (so we know data was actually fetched).\n",
    "We confirm Analysis is called, meaning SPM attempts to do metric computations.\n",
    "We confirm the final indexing into ES (index_document) is invoked with some result data.\n",
    "Test Approach\n",
    "\n",
    "This approach mimics both the style of metrics_spark.py tests (where we verify metric calculations) and the style of db.py/s3_db tests (mocking out DB interactions).\n",
    "For a true integration test, you could remove the mocking of DB and index_document so the code actually reads from S3 (or a real data source) and writes to Elasticsearch. That would require real credentials and network connectivity, so typically those tests are labeled as integration or end-to-end tests.\n",
    "Running the Tests\n",
    "\n",
    "Make sure pytest is installed (pip install pytest).\n",
    "Save this file (e.g., test_spm.py) in your tests directory and run:\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pytest test_spm.py\n",
    "If you prefer to separate out integration vs. unit tests, you could mark one with @pytest.mark.integration and run them selectively.\n",
    "This structure demonstrates how to test:\n",
    "\n",
    "(a) Ground-truth metrics: ensuring your code runs the path that loads ground-truth data and calls analysis.db_gt_metrics_spark or similar.\n",
    "(b) Non–ground-truth metrics: ensuring your code runs the path for baseline comparison, etc.\n",
    "Feel free to add or remove details according to your actual code paths, class names, and how your pipeline is set up.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
