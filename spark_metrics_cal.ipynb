{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, count as cnt, lit, udf\n",
    "from pyspark.sql.types import IntegerType, StringType, DoubleType, StructType, StructField\n",
    "import math\n",
    "\n",
    "def get_bin_edges(df, column: str, bins: int = 10):\n",
    "    \"\"\"\n",
    "    Derive up to `bins` quantile-based boundaries for `column`.\n",
    "    - Excludes null rows from boundary derivation.\n",
    "    - Returns a sorted list of unique boundaries (including min and max).\n",
    "    - If all values are identical or no non-null data, returns a single boundary list.\n",
    "    \"\"\"\n",
    "    non_null = df.filter(col(column).isNotNull())\n",
    "    total_non_null = non_null.count()\n",
    "    if total_non_null == 0:\n",
    "        # All values are null\n",
    "        return []\n",
    "\n",
    "    stats = non_null.agg(F.min(col(column)).alias(\"min_val\"), F.max(col(column)).alias(\"max_val\")).collect()[0]\n",
    "    min_val, max_val = stats.min_val, stats.max_val\n",
    "    if min_val == max_val:\n",
    "        # Single unique value\n",
    "        return [float(min_val)]\n",
    "\n",
    "    fractiles = [i / bins for i in range(1, bins)]\n",
    "    quantiles = non_null.approxQuantile(column, fractiles, 0.001)\n",
    "    raw_edges = [min_val] + quantiles + [max_val]\n",
    "    unique_edges = sorted(list(set(raw_edges)))\n",
    "    return unique_edges\n",
    "\n",
    "def build_bin_assigner(bin_edges: list):\n",
    "    \"\"\"\n",
    "    Return a UDF that assigns each numeric value to a labeled bin:\n",
    "    - `\"null_bin\"` if value is None\n",
    "    - `\"oor_low\"` if value < bin_edges[0]\n",
    "    - `\"oor_high\"` if value > bin_edges[-1]\n",
    "    - Otherwise, `\"bin_i\"` for each interval [left, right]\n",
    "    The final bin is inclusive of right edge to accommodate max value.\n",
    "    \"\"\"\n",
    "    if not bin_edges or len(bin_edges) <= 1:\n",
    "        # Single-value or no edges scenario\n",
    "        def single_bin(value):\n",
    "            if value is None:\n",
    "                return \"null_bin\"\n",
    "            return \"bin_0\"\n",
    "\n",
    "        return udf(single_bin, StringType())\n",
    "\n",
    "    def assign_bin(value):\n",
    "        if value is None:\n",
    "            return \"null_bin\"\n",
    "        if value < bin_edges[0] or value > bin_edges[-1]:\n",
    "            return \"oor\"\n",
    "\n",
    "        for i in range(len(bin_edges) - 1):\n",
    "            left = bin_edges[i]\n",
    "            right = bin_edges[i + 1]\n",
    "            if i == len(bin_edges) - 2:\n",
    "                # Last bin includes the right edge\n",
    "                if value >= left and value <= right:\n",
    "                    return f\"bin_{i}\"\n",
    "            else:\n",
    "                # Half-open interval\n",
    "                if value >= left and value < right:\n",
    "                    return f\"bin_{i}\"\n",
    "\n",
    "        return f\"bin_{len(bin_edges) - 2}\"  # Fallback\n",
    "\n",
    "    return udf(assign_bin, StringType())\n",
    "\n",
    "def build_histogram(df, column: str, bins: int = 10):\n",
    "    \"\"\"\n",
    "    Create a histogram for a numeric column with up to `bins` decile boundaries.\n",
    "    Output columns:\n",
    "    - column_name\n",
    "    - bin_label\n",
    "    - bin_range\n",
    "    - count_in_bin\n",
    "    - proportion\n",
    "    \"\"\"\n",
    "    edges = get_bin_edges(df, column, bins)\n",
    "    assigner = build_bin_assigner(edges)\n",
    "\n",
    "    # Assign each row to a bin\n",
    "    binned = df.withColumn(\"bin_label\", assigner(col(column)))\n",
    "    total_rows = binned.count()\n",
    "    if total_rows == 0:\n",
    "        # No data\n",
    "        empty_schema = StructType([\n",
    "            StructField(\"column_name\", StringType(), True),\n",
    "            StructField(\"bin_label\", StringType(), True),\n",
    "            StructField(\"bin_range\", StringType(), True),\n",
    "            StructField(\"count_in_bin\", IntegerType(), True),\n",
    "            StructField(\"proportion\", DoubleType(), True),\n",
    "        ])\n",
    "        return spark.createDataFrame([], empty_schema)\n",
    "\n",
    "    # Count how many rows per bin\n",
    "    bin_counts = binned.groupBy(\"bin_label\").agg(cnt(\"*\").alias(\"count_in_bin\"))\n",
    "\n",
    "    # Construct human-readable bin ranges for reference\n",
    "    bin_meta = []\n",
    "    if len(edges) <= 1:\n",
    "        # Single bin or no edges\n",
    "        bin_meta.append((\"bin_0\", \"[SingleValue]\"))\n",
    "    else:\n",
    "        for i in range(len(edges) - 1):\n",
    "            left = edges[i]\n",
    "            right = edges[i + 1]\n",
    "            if i == len(edges) - 2:\n",
    "                # Last bin is inclusive\n",
    "                label = f\"[{left}, {right}]\"\n",
    "            else:\n",
    "                label = f\"[{left}, {right})\"\n",
    "            bin_meta.append((f\"bin_{i}\", label))\n",
    "\n",
    "    # out-of-range & null\n",
    "    bin_meta.append((\"oor\", \"Out of Range\"))\n",
    "    bin_meta.append((\"null_bin\", \"NULL\"))\n",
    "\n",
    "    # Convert bin_meta -> Spark DataFrame for an easy join\n",
    "    meta_df = spark.createDataFrame(\n",
    "        [(column, x[0], x[1]) for x in bin_meta],\n",
    "        [\"column_name\", \"bin_label\", \"bin_range\"]\n",
    "    )\n",
    "\n",
    "    # Join counts\n",
    "    hist_df = meta_df \\\n",
    "        .join(bin_counts, on=\"bin_label\", how=\"left\") \\\n",
    "        .fillna({\"count_in_bin\": 0}) \\\n",
    "        .withColumn(\"proportion\", col(\"count_in_bin\") / lit(total_rows)) \\\n",
    "        .select(\"column_name\", \"bin_label\", \"bin_range\", \"count_in_bin\", \"proportion\")\n",
    "\n",
    "    return hist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, when, lit, count as _count, sum as _sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "import math\n",
    "\n",
    "# Split DataFrame: Baseline vs. Comparison\n",
    "baseline_df, comparison_df = df_res.randomSplit([0.5, 0.5], seed=42)\n",
    "print(\"Baseline Count:\", baseline_df.count())\n",
    "print(\"Comparison Count:\", comparison_df.count())\n",
    "\n",
    "#######################################################\n",
    "# 4. Build bins + baseline proportions\n",
    "#######################################################\n",
    "\n",
    "def get_bins_proportions_for_baseline(df_baseline, col_name, num_bins=10):\n",
    "    \"\"\"Auto-derive bin edges and baseline proportions from baseline data.\"\"\"\n",
    "    stats = df_baseline.agg(\n",
    "        F.min(col_name).alias(\"minVal\"),\n",
    "        F.max(col_name).alias(\"maxVal\")\n",
    "    ).collect()[0]\n",
    "    min_val = stats[\"minVal\"]\n",
    "    max_val = stats[\"maxVal\"]\n",
    "\n",
    "    # Handle edge cases\n",
    "    if min_val is None or max_val is None:\n",
    "        return None, None\n",
    "    if min_val == max_val:\n",
    "        # everything is the same -> single bin\n",
    "        return [min_val, min_val + 1e-9], [1.0]\n",
    "\n",
    "    # Evenly spaced bins\n",
    "    step = (max_val - min_val) / num_bins\n",
    "    edges = [min_val + i * step for i in range(num_bins)] + [max_val + 1e-9]\n",
    "\n",
    "    # Bin assignment UDF\n",
    "    def binning_func(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        for i in range(num_bins):\n",
    "            if i < num_bins - 1:\n",
    "                if x >= edges[i] and x < edges[i+1]:\n",
    "                    return i\n",
    "            else:\n",
    "                # last bin includes top\n",
    "                if x >= edges[i] and x <= edges[i+1]:\n",
    "                    return i\n",
    "        return None\n",
    "\n",
    "    bin_udf = F.udf(binning_func, IntegerType())\n",
    "    binned = df_baseline.withColumn(\"bin_index\", bin_udf(col(col_name)))\n",
    "\n",
    "    total_count = binned.count()\n",
    "    if total_count == 0:\n",
    "        # no records\n",
    "        return edges, [0] * num_bins\n",
    "\n",
    "    bin_counts = (binned.groupBy(\"bin_index\")\n",
    "                  .agg(_count(\"*\").alias(\"cnt\"))\n",
    "                  .collect())\n",
    "    count_dict = {r[\"bin_index\"]: r[\"cnt\"] for r in bin_counts}\n",
    "\n",
    "    proportions = []\n",
    "    for i in range(num_bins):\n",
    "        cnt = count_dict.get(i, 0)\n",
    "        proportions.append(cnt / total_count)\n",
    "\n",
    "    return edges, proportions\n",
    "\n",
    "# Decide which columns to measure for VSI, plus model_score for PSI\n",
    "input_cols = [\n",
    "    \"tran_amt\",\n",
    "    \"sec_since_dvc_use_last_ts\",\n",
    "    \"sec_since_add_payee_rqst_in_1\",\n",
    "    \"sec_since_prty_birth_inc_dt\"\n",
    "]\n",
    "\n",
    "all_cols_for_bins = input_cols + [\"model_score\"]\n",
    "\n",
    "baseline_bins_info = {}\n",
    "for c in all_cols_for_bins:\n",
    "    bins, base_props = get_bins_proportions_for_baseline(baseline_df, c, num_bins=5)\n",
    "    baseline_bins_info[c] = {\n",
    "        \"bins\": bins,\n",
    "        \"base_props\": base_props\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Compute VSI for inputs, PSI for model_score\n",
    "def psi(df_comp, column, bins, baseline_props):\n",
    "    \"\"\"Compute Stability Index for df_comp vs. baseline distribution.\"\"\"\n",
    "    if not bins or not baseline_props or len(bins) == 1:\n",
    "        return 0.0  # degenerate case or empty\n",
    "\n",
    "    num_bins = len(baseline_props)\n",
    "\n",
    "    # Binning UDF\n",
    "    def binning_func(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        for i in range(num_bins):\n",
    "            if i < num_bins - 1:\n",
    "                if x >= bins[i] and x < bins[i+1]:\n",
    "                    return i\n",
    "            else:\n",
    "                if x >= bins[i] and x <= bins[i+1]:\n",
    "                    return i\n",
    "        return None\n",
    "\n",
    "    bin_udf = F.udf(binning_func, IntegerType())\n",
    "    binned_df = df_comp.withColumn(\"bin_index\", bin_udf(col(column)))\n",
    "\n",
    "    total_count = binned_df.count()\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    bin_counts = (binned_df.groupBy(\"bin_index\")\n",
    "                  .agg(_count(\"*\").alias(\"cnt\"))\n",
    "                  .collect())\n",
    "    count_dict = {r[\"bin_index\"]: r[\"cnt\"] for r in bin_counts}\n",
    "\n",
    "    # observed proportions\n",
    "    obs_props = []\n",
    "    for i in range(num_bins):\n",
    "        cnt = count_dict.get(i, 0)\n",
    "        obs_props.append(cnt / total_count)\n",
    "\n",
    "    # SI sum\n",
    "    si_val = 0.0\n",
    "    for obs, base in zip(obs_props, baseline_props):\n",
    "        if obs > 0 and base > 0:\n",
    "            si_val += (obs - base) * math.log(obs / base)\n",
    "\n",
    "    return si_val\n",
    "\n",
    "# Calculate VSI/PSI\n",
    "vsi_psi_results = {}\n",
    "for c in input_cols:\n",
    "    bins = baseline_bins_info[c][\"bins\"]\n",
    "    base_props = baseline_bins_info[c][\"base_props\"]\n",
    "    psi_val = psi(comparison_df, c, bins, base_props)\n",
    "    vsi_psi_results[f\"VSI_{c}\"] = psi_val\n",
    "\n",
    "# PSI for model_score\n",
    "score_bins = baseline_bins_info[\"model_score\"][\"bins\"]\n",
    "score_props = baseline_bins_info[\"model_score\"][\"base_props\"]\n",
    "psi_val = psi(comparison_df, \"model_score\", score_bins, score_props)\n",
    "vsi_psi_results[\"PSI_model_score\"] = psi_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VSI for inputs, PSI for model_score\n",
    "def psi(df_comp, column, bins, baseline_props):\n",
    "    \"\"\"Compute Stability Index for df_comp vs. baseline distribution.\"\"\"\n",
    "    if not bins or not baseline_props or len(bins) == 1:\n",
    "        return 0.0  # degenerate case or empty\n",
    "\n",
    "    num_bins = len(baseline_props)\n",
    "\n",
    "    # Binning UDF\n",
    "    def binning_func(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        for i in range(num_bins):\n",
    "            if i < num_bins - 1:\n",
    "                if x >= bins[i] and x < bins[i+1]:\n",
    "                    return i\n",
    "            else:\n",
    "                if x >= bins[i] and x <= bins[i+1]:\n",
    "                    return i\n",
    "        return None\n",
    "\n",
    "    bin_udf = F.udf(binning_func, IntegerType())\n",
    "    binned_df = df_comp.withColumn(\"bin_index\", bin_udf(col(column)))\n",
    "\n",
    "    total_count = binned_df.count()\n",
    "    if total_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    bin_counts = (binned_df.groupBy(\"bin_index\")\n",
    "                  .agg(_count(\"*\").alias(\"cnt\"))\n",
    "                  .collect())\n",
    "    count_dict = {r[\"bin_index\"]: r[\"cnt\"] for r in bin_counts}\n",
    "\n",
    "    # observed proportions\n",
    "    obs_props = []\n",
    "    for i in range(num_bins):\n",
    "        cnt = count_dict.get(i, 0)\n",
    "        obs_props.append(cnt / total_count)\n",
    "\n",
    "    # SI sum\n",
    "    si_val = 0.0\n",
    "    for obs, base in zip(obs_props, baseline_props):\n",
    "        if obs > 0 and base > 0:\n",
    "            si_val += (obs - base) * math.log(obs / base)\n",
    "\n",
    "    return si_val\n",
    "\n",
    "# Calculate VSI/PSI\n",
    "vsi_psi_results = {}\n",
    "for c in input_cols:\n",
    "    bins = baseline_bins_info[c][\"bins\"]\n",
    "    base_props = baseline_bins_info[c][\"base_props\"]\n",
    "    psi_val = psi(comparison_df, c, bins, base_props)\n",
    "    vsi_psi_results[f\"VSI_{c}\"] = psi_val\n",
    "\n",
    "# PSI for model_score\n",
    "score_bins = baseline_bins_info[\"model_score\"][\"bins\"]\n",
    "score_props = baseline_bins_info[\"model_score\"][\"base_props\"]\n",
    "psi_val = psi(comparison_df, \"model_score\", score_bins, score_props)\n",
    "vsi_psi_results[\"PSI_model_score\"] = psi_val\n",
    "\n",
    "##################################################\n",
    "# Compute Performance KPIs (VDR, TDR, TFPR, KS)\n",
    "##################################################\n",
    "def compute_performance_kpis(df_comp, threshold=0.5):\n",
    "    \"\"\"Compute VDR, TDR, TFPR with a given score threshold.\"\"\"\n",
    "    pred_df = df_comp.withColumn(\n",
    "        \"pred_fraud\",\n",
    "        when(col(\"model_score\") >= threshold, 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    # Value Detection Rate (VDR)\n",
    "    sum_fraud_amt = pred_df.filter(col(\"frd_tag\") == 1).agg(_sum(\"tran_amt\")).collect()[0][0]\n",
    "    sum_detected_fraud_amt = pred_df.filter((col(\"frd_tag\") == 1) & (col(\"pred_fraud\") == 1)) \\\n",
    "        .agg(_sum(\"tran_amt\")).collect()[0][0]\n",
    "\n",
    "    if not sum_fraud_amt or sum_fraud_amt == 0:\n",
    "        vdr = 0.0\n",
    "    else:\n",
    "        vdr = float(sum_detected_fraud_amt or 0) / float(sum_fraud_amt)\n",
    "\n",
    "    # TDR\n",
    "    count_fraud = pred_df.filter(col(\"frd_tag\") == 1).count()\n",
    "    count_detected_fraud = pred_df.filter((col(\"frd_tag\") == 1) & (col(\"pred_fraud\") == 1)).count()\n",
    "    tdr = float(count_detected_fraud) / float(count_fraud) if count_fraud > 0 else 0.0\n",
    "\n",
    "    # TFPR\n",
    "    count_pred_fraud = pred_df.filter(col(\"pred_fraud\") == 1).count()\n",
    "    count_false_pos = pred_df.filter((col(\"frd_tag\") == 0) & (col(\"pred_fraud\") == 1)).count()\n",
    "    tfpr = float(count_false_pos) / float(count_pred_fraud) if count_pred_fraud > 0 else 0.0\n",
    "\n",
    "    return {\"VDR\": vdr, \"TDR\": tdr, \"TFPR\": tfpr}\n",
    "\n",
    "def compute_ks(df_comp):\n",
    "    \"\"\"Compute KS statistic between frd_tag=1 and frd_tag=0 distributions of model_score.\"\"\"\n",
    "    pdf = df_comp.select(\"model_score\", \"frd_tag\").toPandas()\n",
    "    fraud_scores = pdf[pdf[\"frd_tag\"] == 1][\"model_score\"].sort_values()\n",
    "    non_fraud_scores = pdf[pdf[\"frd_tag\"] == 0][\"model_score\"].sort_values()\n",
    "\n",
    "    n_fraud = len(fraud_scores)\n",
    "    n_non_fraud = len(non_fraud_scores)\n",
    "    if n_fraud == 0 or n_non_fraud == 0:\n",
    "        return 0.0\n",
    "\n",
    "    all_scores = sorted(pdf[\"model_score\"].unique())\n",
    "    fraud_idx = 0\n",
    "    non_fraud_idx = 0\n",
    "    cdf_diff = 0.0\n",
    "\n",
    "    for s in all_scores:\n",
    "        while fraud_idx < n_fraud and fraud_scores.iloc[fraud_idx] <= s:\n",
    "            fraud_idx += 1\n",
    "        while non_fraud_idx < n_non_fraud and non_fraud_scores.iloc[non_fraud_idx] <= s:\n",
    "            non_fraud_idx += 1\n",
    "\n",
    "        f_cdf = fraud_idx / n_fraud\n",
    "        nf_cdf = non_fraud_idx / n_non_fraud\n",
    "        diff = abs(f_cdf - nf_cdf)\n",
    "        cdf_diff = max(cdf_diff, diff)\n",
    "\n",
    "    return cdf_diff\n",
    "\n",
    "perf_results = compute_performance_kpis(comparison_df, threshold=0.5)\n",
    "ks_val = compute_ks(comparison_df)\n",
    "\n",
    "##################################################\n",
    "# Combine & Print Final KPIs\n",
    "##################################################\n",
    "final_metrics = {}\n",
    "final_metrics.update(vsi_psi_results)  # VSI_..., PSI_model_score\n",
    "final_metrics.update(perf_results)  # VDR, TDR, TFPR\n",
    "final_metrics[\"KS\"] = ks_val  # KS\n",
    "\n",
    "print(\"Final KPI Metrics:\")\n",
    "for k, v in final_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASELINE HIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, count as cnt, udf\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType,\n",
    "    DoubleType, IntegerType\n",
    ")\n",
    "import math\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#############################\n",
    "# 1) get_edges: Decile Breakpoints\n",
    "#############################\n",
    "def get_edges(df: DataFrame, col_name: str, nbins: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Uses approxQuantile to derive decile-based edges for `col_name`.\n",
    "    Returns a sorted list of unique numeric edges:\n",
    "      [min_val, q1, q2, ..., max_val]\n",
    "    \"\"\"\n",
    "    valid = df.filter(col(col_name).isNotNull())\n",
    "    cnt_valid = valid.count()\n",
    "    if cnt_valid == 0:\n",
    "        return []\n",
    "\n",
    "    stats = valid.agg(\n",
    "        F.min(col(col_name)).alias(\"mn\"),\n",
    "        F.max(col(col_name)).alias(\"mx\")\n",
    "    ).collect()[0]\n",
    "    mn, mx = stats.mn, stats.mx\n",
    "    if mn == mx:\n",
    "        # Single unique value\n",
    "        return [float(mn)]\n",
    "\n",
    "    fractiles = [i / nbins for i in range(1, nbins)]\n",
    "    quants = valid.approxQuantile(col_name, fractiles, 0.001)\n",
    "    raw_edges = [mn] + quants + [mx]\n",
    "    return sorted(set(raw_edges))\n",
    "\n",
    "#############################\n",
    "# 2) get_assigner: UDF for Binning\n",
    "#############################\n",
    "def get_assigner(edges: list):\n",
    "    \"\"\"\n",
    "    Returns a UDF that maps numeric values to:\n",
    "      - 'bin_X' for each interval\n",
    "      - 'null_bin' if None\n",
    "      - 'oor' if out of [edges[0], edges[-1]]\n",
    "    \"\"\"\n",
    "    if not edges or len(edges) <= 1:\n",
    "        def single_bin(x):\n",
    "            if x is None:\n",
    "                return \"null_bin\"\n",
    "            return \"bin_0\"\n",
    "        return udf(single_bin, StringType())\n",
    "\n",
    "    def assign_bin(x):\n",
    "        if x is None:\n",
    "            return \"null_bin\"\n",
    "        if x < edges[0] or x > edges[-1]:\n",
    "            return \"oor\"\n",
    "        for i in range(len(edges) - 1):\n",
    "            left, right = edges[i], edges[i+1]\n",
    "            # last bin inclusive\n",
    "            if i == len(edges) - 2:\n",
    "                if left <= x <= right:\n",
    "                    return f\"bin_{i}\"\n",
    "            else:\n",
    "                if left <= x < right:\n",
    "                    return f\"bin_{i}\"\n",
    "        return f\"bin_{len(edges) - 2}\"  # fallback\n",
    "    return udf(assign_bin, StringType())\n",
    "\n",
    "#############################\n",
    "# 3) get_hist: Build a Decile-Based Histogram for One Column\n",
    "#############################\n",
    "def get_hist(\n",
    "    df: DataFrame,\n",
    "    col_name: str,\n",
    "    nbins: int = 10,\n",
    "    edges: list = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Creates a histogram for `df[col_name]`:\n",
    "      - If edges is None, derive decile edges from df (baseline mode).\n",
    "      - If edges is provided, reuse them (new data mode).\n",
    "\n",
    "    Returns a dict:\n",
    "    {\n",
    "      \"edges\": [...],\n",
    "      \"bins\": {\n",
    "        \"bin_0\": {\"range\": ..., \"cnt\":..., \"prop\":..., \"adj_prop\":...},\n",
    "        \"bin_1\": {...},\n",
    "        \"null_bin\": {...},\n",
    "        \"oor\": {...}\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    # 1) Determine edges\n",
    "    if edges is None:\n",
    "        edges = get_edges(df, col_name, nbins)\n",
    "\n",
    "    # 2) Assign bins\n",
    "    assigner = get_assigner(edges)\n",
    "    binned_df = df.withColumn(\"bin\", assigner(col(col_name)))\n",
    "    total_rows = binned_df.count()\n",
    "    if total_rows == 0:\n",
    "        return {\n",
    "            \"edges\": edges,\n",
    "            \"bins\": {\n",
    "                \"null_bin\": {\n",
    "                    \"range\": \"NULL\",\n",
    "                    \"cnt\": 0,\n",
    "                    \"prop\": 0.0,\n",
    "                    \"adj_prop\": 0.0\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    # 3) Count per bin\n",
    "    bin_counts = binned_df.groupBy(\"bin\").agg(cnt(\"*\").alias(\"cnt\")).collect()\n",
    "    cnt_map = {r[\"bin\"]: r[\"cnt\"] for r in bin_counts}\n",
    "\n",
    "    # 4) prop & adj_prop\n",
    "    null_cnt = cnt_map.get(\"null_bin\", 0)\n",
    "    oor_cnt  = cnt_map.get(\"oor\", 0)\n",
    "    valid_cnt = sum(v for k, v in cnt_map.items() if k.startswith(\"bin_\"))\n",
    "\n",
    "    null_prop = null_cnt / total_rows\n",
    "    oor_prop  = oor_cnt / total_rows\n",
    "    valid_prop = 1.0 - null_prop - oor_prop\n",
    "\n",
    "    # 5) Build \"bins\" dictionary\n",
    "    bins_data = {}\n",
    "\n",
    "    # If edges is empty or len=1 => only bin_0\n",
    "    if len(edges) <= 1:\n",
    "        # single bin\n",
    "        label = \"bin_0\"\n",
    "        cnt_val = cnt_map.get(label, 0)\n",
    "        bins_data[label] = {\n",
    "            \"range\": \"[SingleValue]\",\n",
    "            \"cnt\": cnt_val,\n",
    "            \"prop\": cnt_val / total_rows if total_rows else 0.0,\n",
    "            \"adj_prop\": (cnt_val / valid_cnt * valid_prop) if valid_cnt else 0.0\n",
    "        }\n",
    "    else:\n",
    "        # Multiple intervals\n",
    "        for i in range(len(edges) - 1):\n",
    "            label = f\"bin_{i}\"\n",
    "            left, right = edges[i], edges[i + 1]\n",
    "            if i == (len(edges) - 2):\n",
    "                rng_label = f\"[{left}, {right}]\"\n",
    "            else:\n",
    "                rng_label = f\"[{left}, {right})\"\n",
    "            cnt_val = cnt_map.get(label, 0)\n",
    "            prp = cnt_val / total_rows\n",
    "            adj_prp = (cnt_val / valid_cnt * valid_prop) if valid_cnt else 0.0\n",
    "            bins_data[label] = {\n",
    "                \"range\": rng_label,\n",
    "                \"cnt\": cnt_val,\n",
    "                \"prop\": prp,\n",
    "                \"adj_prop\": adj_prp\n",
    "            }\n",
    "\n",
    "    # Add null_bin & oor\n",
    "    bins_data[\"null_bin\"] = {\n",
    "        \"range\": \"NULL\",\n",
    "        \"cnt\": null_cnt,\n",
    "        \"prop\": null_prop,\n",
    "        \"adj_prop\": null_prop\n",
    "    }\n",
    "    bins_data[\"oor\"] = {\n",
    "        \"range\": \"Out of Range\",\n",
    "        \"cnt\": oor_cnt,\n",
    "        \"prop\": oor_prop,\n",
    "        \"adj_prop\": oor_prop\n",
    "    }\n",
    "\n",
    "    return {\"edges\": edges, \"bins\": bins_data}\n",
    "\n",
    "#############################\n",
    "# 4) get_baseline_hists (Multiple Features)\n",
    "#############################\n",
    "def get_baseline_hists(\n",
    "    df: DataFrame,\n",
    "    features: list,\n",
    "    nbins: int = 10\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Builds a dictionary of baseline histograms for multiple features:\n",
    "    {\n",
    "      \"feat_1\": {\n",
    "        \"edges\": [...],\n",
    "        \"bins\": {...}\n",
    "      },\n",
    "      \"feat_2\": {...},\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for feat in features:\n",
    "        hist_info = get_hist(df, feat, nbins=nbins, edges=None)\n",
    "        result[feat] = hist_info\n",
    "    return result\n",
    "\n",
    "#############################\n",
    "# 5) save_baseline_json / load_baseline_json\n",
    "#############################\n",
    "def save_baseline_json(hist_dict: dict, file_path: str):\n",
    "    \"\"\"\n",
    "    Saves the multi-feature dictionary as JSON:\n",
    "    {\n",
    "      \"feat_1\": { \"edges\": [...], \"bins\": {...} },\n",
    "      \"feat_2\": {...}\n",
    "    }\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(hist_dict, f)\n",
    "\n",
    "def load_baseline_json(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads the multi-feature histogram dictionary from JSON.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "#############################\n",
    "# 6) calc_psi: Re-bin new data & compare with baseline\n",
    "#############################\n",
    "def calc_psi(\n",
    "    new_df: DataFrame,\n",
    "    col_name: str,\n",
    "    baseline_hist: dict\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Hard-coded epsilon. \n",
    "    1) Extract edges from baseline_hist.\n",
    "    2) Build new histogram with those edges.\n",
    "    3) Compare 'adj_prop' per bin => sum( (p_new - p_base)*ln(p_new/p_base) ).\n",
    "    \"\"\"\n",
    "    EPSILON = 1e-10\n",
    "    # 1) Get the baseline edges\n",
    "    edges = baseline_hist[\"edges\"]\n",
    "\n",
    "    # 2) Build new distribution\n",
    "    new_hist = get_hist(new_df, col_name, edges=edges)\n",
    "\n",
    "    # 3) Compare\n",
    "    base_bins = baseline_hist[\"bins\"]\n",
    "    new_bins  = new_hist[\"bins\"]\n",
    "    all_bins = set(base_bins.keys()) | set(new_bins.keys())\n",
    "\n",
    "    psi_val = 0.0\n",
    "    for b in all_bins:\n",
    "        # baseline & new => adj_prop\n",
    "        p_base = base_bins.get(b, {}).get(\"adj_prop\", 0.0) + EPSILON\n",
    "        p_new  = new_bins.get(b, {}).get(\"adj_prop\", 0.0) + EPSILON\n",
    "        psi_val += (p_new - p_base) * math.log(p_new / p_base)\n",
    "\n",
    "    return psi_val\n",
    "\n",
    "##############################################\n",
    "# EXAMPLE USAGE (Uncomment to Run)\n",
    "##############################################\n",
    "\"\"\"\n",
    "def example_usage():\n",
    "    # 1) Sample DataFrame with multiple features\n",
    "    data = [\n",
    "        (1.0, 10.0, 5.0),\n",
    "        (2.5, 20.0, None),\n",
    "        (None, 30.0, 2.5),\n",
    "        (100.0, 40.0, 10.0),\n",
    "        (9999.0, 50.0, 60.0),\n",
    "        (2.0, None, None),\n",
    "    ]\n",
    "    df = spark.createDataFrame(data, [\"feat_1\", \"feat_2\", \"feat_3\"])\n",
    "\n",
    "    # 2) Split => baseline & new\n",
    "    df_baseline, df_new = df.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "    # 3) Build baseline histogram for multiple features\n",
    "    features = [\"feat_1\", \"feat_2\", \"feat_3\"]\n",
    "    baseline_dict = get_baseline_hists(df_baseline, features, nbins=5)\n",
    "\n",
    "    # 4) Save baseline to JSON\n",
    "    save_baseline_json(baseline_dict, \"baseline_histograms.json\")\n",
    "\n",
    "    # 5) Load baseline from JSON\n",
    "    loaded_baseline = load_baseline_json(\"baseline_histograms.json\")\n",
    "\n",
    "    # 6) Calculate PSI for each feature\n",
    "    for feat in features:\n",
    "        psi_val = calc_psi(df_new, feat, loaded_baseline[feat])\n",
    "        print(f\"PSI for {feat}: {psi_val}\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
